{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "file_name = 'talk_tags.json'\n",
    "\n",
    "from google.colab import files\n",
    "#uploaded = files.upload()  # Это позволит загрузить файл через интерфейс Google Colab\n",
    "\n",
    "file_name = 'talk_tags.json'\n",
    "\n",
    "# Чтение JSON файла\n",
    "with open(file_name, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Преобразуем JSON данные в DataFrame для удобной работы\n",
    "df = pd.json_normalize(data)\n",
    "\n",
    "# Проверим первые несколько строк данных\n",
    "df.head(6)\n",
    "\n",
    "type(df['text'][0])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "    # Преобразование всех элементов колонки text из списка в строку\n",
    "df['text'] = df['text'].apply(lambda x: ','.join(map(str, x)))\n",
    "\n",
    "type(df['text'][0])\n",
    "\n",
    "# Инициализация инструментов NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "\n",
    "# Функция предобработки текста\n",
    "def preprocess_text(text):\n",
    "    # Приведение к нижнему регистру\n",
    "    text = text.lower()\n",
    "\n",
    "    text = text.replace(',', ' ')\n",
    "\n",
    "    # Удаление лишних символов, которые не являются словами\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Удаляем пунктуацию, кроме слов и пробелов\n",
    "\n",
    "    # Токенизация текста\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Лемматизация\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Удаление пустых строк и коротких токенов\n",
    "    clean_tokens = [token for token in lemmatized_tokens if len(token) > 1]\n",
    "\n",
    "    return ' '.join(clean_tokens)\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "df['cleaned_text']\n",
    "\n",
    "# Список слов, которые нужно удалить\n",
    "words_to_remove = ['ctxt', 'str', 'text']\n",
    "\n",
    "# Функция для удаления лишних слов\n",
    "def remove_unwanted_words(text):\n",
    "    # Разбиваем текст на слова и фильтруем\n",
    "    clean_tokens = [word for word in text.split() if word not in words_to_remove]\n",
    "    return ' '.join(clean_tokens)\n",
    "\n",
    "# Применяем функцию к колонке 'cleaned_text'\n",
    "df['cleaned_text_after'] = df['cleaned_text'].apply(remove_unwanted_words)\n",
    "\n",
    "# Вывод первых строк\n",
    "print(df['cleaned_text_after'].head())\n",
    "\n",
    "# Список слов, которые нужно удалить\n",
    "words_to_remove = ['ctxt', 'str', 'text']\n",
    "\n",
    "# Функция для удаления лишних слов\n",
    "def remove_unwanted_words(text):\n",
    "    # Разбиваем текст на слова и фильтруем\n",
    "    clean_tokens = [word for word in text.split() if word not in words_to_remove]\n",
    "    return ' '.join(clean_tokens)\n",
    "\n",
    "# Применяем функцию к колонке 'cleaned_text'\n",
    "df['cleaned_text_after'] = df['cleaned_text'].apply(remove_unwanted_words)\n",
    "\n",
    "# Вывод первых строк\n",
    "print(df['cleaned_text_after'].head())\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Токенизация: разбиваем текст на слова\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "# Создание словаря (включает специальные токены)\n",
    "def build_vocab(texts, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        counter.update(tokenize(text))\n",
    "\n",
    "    # Фильтруем редкие слова\n",
    "    vocab = {word: idx + 2 for idx, (word, count) in enumerate(counter.items()) if count >= min_freq}\n",
    "    vocab['<PAD>'] = 0\n",
    "    vocab['<UNK>'] = 1\n",
    "    return vocab\n",
    "\n",
    "# Пример создания словаря\n",
    "texts = df['cleaned_text_after'].tolist()\n",
    "vocab = build_vocab(texts)\n",
    "print(\"Размер словаря:\", len(vocab))\n",
    "\n",
    "\n",
    "# Функция преобразования текста в индексы\n",
    "def text_to_indices(text, vocab):\n",
    "    return [vocab.get(word, vocab['<UNK>']) for word in tokenize(text)]\n",
    "\n",
    "# Преобразуем все тексты в индексы\n",
    "df['text_indices'] = df['cleaned_text_after'].apply(lambda x: text_to_indices(x, vocab))\n",
    "\n",
    "# Пример\n",
    "print(df['text_indices'].head())\n",
    "\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Преобразуем тексты в тензоры с паддингом\n",
    "def collate_fn(batch):\n",
    "    batch = [torch.tensor(item) for item in batch]\n",
    "    return pad_sequence(batch, batch_first=True, padding_value=0)\n",
    "\n",
    "# Делим данные на train, val, test\n",
    "train_texts, test_texts = train_test_split(df['text_indices'].tolist(), test_size=0.2, random_state=42)\n",
    "\n",
    "train_loader = DataLoader(train_texts, batch_size=32, collate_fn=collate_fn, shuffle=True)\n",
    "test_loader = DataLoader(test_texts, batch_size=32, collate_fn=collate_fn)\n",
    "\n",
    "print(\"Размер тренировочного набора:\", len(train_loader.dataset))\n",
    "print(\"Размер тестового набора:\", len(test_loader.dataset))\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Transformer\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Гиперпараметры\n",
    "vocab_size = len(vocab)  # Размер словаря\n",
    "embed_dim = 128          # Размерность эмбеддингов\n",
    "nhead = 4                # Количество \"голов\" в multi-head attention\n",
    "hidden_dim = 256         # Размер скрытых слоёв (feed-forward)\n",
    "num_layers = 2           # Количество энкодеров в Transformer\n",
    "dropout = 0.1            # Дропаут\n",
    "lr = 0.001               # Скорость обучения\n",
    "num_epochs = 10          # Количество эпох\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, nhead, hidden_dim, num_layers, dropout):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, dropout)\n",
    "        self.transformer = Transformer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=hidden_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src)  # Входной текст\n",
    "        tgt = self.embedding(tgt)  # Текст, который нужно предсказать\n",
    "        src = self.pos_encoder(src)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        output = self.transformer(src, tgt)\n",
    "        output = self.fc_out(output)  # Прогноз следующего токена\n",
    "        return output\n",
    "\n",
    "# Позиционное кодирование для добавления информации о порядке слов\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Сдвигаем текст для обучения\n",
    "def create_src_tgt_pairs(data, seq_len=10):\n",
    "    src = []\n",
    "    tgt = []\n",
    "    for sentence in data:\n",
    "        for i in range(1, len(sentence)):\n",
    "            src.append(sentence[:i])\n",
    "            tgt.append(sentence[1:i+1])\n",
    "    return src, tgt\n",
    "\n",
    "# Преобразуем данные\n",
    "data = df['text_indices'].tolist()\n",
    "src_data, tgt_data = create_src_tgt_pairs(data)\n",
    "\n",
    "# Паддинг и DataLoader\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_batch = pad_sequence([torch.tensor(x) for x in src_batch], batch_first=True, padding_value=0)\n",
    "    tgt_batch = pad_sequence([torch.tensor(x) for x in tgt_batch], batch_first=True, padding_value=0)\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "train_data, test_data = train_test_split(list(zip(src_data, tgt_data)), test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader(train_data, batch_size=32, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_data, batch_size=32, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "model = TransformerModel(vocab_size, embed_dim, nhead, hidden_dim, num_layers, dropout)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Цикл обучения\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for src, tgt in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt[:, :-1])  # Сдвигаем на 1 для предсказания\n",
    "        loss = criterion(output.reshape(-1, vocab_size), tgt[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), \"model_weights.pth\")\n",
    "\n",
    "\n",
    "# Добавьте '<unk>' в словарь с индексом 0\n",
    "vocab = {word: idx for idx, word in enumerate(vocab)}  # Используйте свой список слов\n",
    "vocab[\"<unk>\"] = 0  # Добавляем токен для неизвестных слов\n",
    "\n",
    "# Обратный словарь\n",
    "reverse_vocab = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "# Токенизатор (преобразует строку в индексы)\n",
    "def tokenizer(text):\n",
    "    return [vocab.get(word, vocab[\"<unk>\"]) for word in text.split()]\n",
    "\n",
    "# Детокенизатор (преобразует индексы обратно в строку)\n",
    "def detokenizer(indices):\n",
    "    return \" \".join([reverse_vocab.get(idx, \"<unk>\") for idx in indices])\n",
    "\n",
    "def generate_text_top_p(model, start_sequence, max_len=50, temperature=1.0, top_p=0.9):\n",
    "    model.eval()\n",
    "    tokens = tokenizer(start_sequence)  # Токенизируем начальную последовательность\n",
    "    input_seq = torch.tensor(tokens).unsqueeze(0)  # Добавляем размер batch\n",
    "\n",
    "    generated = tokens.copy()  # Сохраняем начальную последовательность\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        # Получаем предсказание модели\n",
    "        with torch.no_grad():\n",
    "            output = model(input_seq, input_seq)  # Вход и цель одинаковы при генерации\n",
    "            next_token_logits = output[0, -1, :]  # Последний токен\n",
    "\n",
    "        # Применяем температурное масштабирование\n",
    "        next_token_logits = next_token_logits / temperature\n",
    "\n",
    "        # Применяем top-p сэмплинг\n",
    "        sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        cutoff = torch.where(cumulative_probs > top_p)[0][0]\n",
    "        filtered_logits = sorted_logits[:cutoff + 1]\n",
    "        filtered_indices = sorted_indices[:cutoff + 1]\n",
    "\n",
    "        probs = F.softmax(filtered_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        # Добавляем предсказанный токен к последовательности\n",
    "        generated.append(filtered_indices[next_token].item())\n",
    "        input_seq = torch.tensor(generated).unsqueeze(0)\n",
    "\n",
    "        # Останавливаем генерацию при достижении специального токена (если есть)\n",
    "        if filtered_indices[next_token].item() == vocab.get('<eos>', -1):\n",
    "            break\n",
    "\n",
    "    # Декодируем сгенерированные токены в текст\n",
    "    return detokenizer(generated)\n",
    "\n",
    "# Пример генерации с использованием top-p\n",
    "start_sequence = \"adventurer\"\n",
    "generated_text = generate_text_top_p(model, start_sequence, max_len=30, temperature=0.7, top_p=0.9)\n",
    "print(\"Generated Text:\", generated_text)\n",
    "\n",
    "\n",
    "# Сохранение словаря\n",
    "with open(\"vocab.json\", \"w\") as f:\n",
    "    json.dump(vocab, f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
