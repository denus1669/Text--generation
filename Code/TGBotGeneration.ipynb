{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from telegram import Update, ReplyKeyboardMarkup\n",
    "from telegram.ext import Updater, CommandHandler, MessageHandler, Filters, CallbackContext\n",
    "\n",
    "checkpoint = torch.load(\"/content/model_weights.pth\", map_location=torch.device('cpu'))\n",
    "print(checkpoint.keys())  # Проверим, какие веса сохранены\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "\n",
    "# Определяем ту же модель\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, nhead, hidden_dim, num_layers, dropout):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, dropout)\n",
    "        self.transformer = Transformer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=hidden_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src)\n",
    "        tgt = self.embedding(tgt)\n",
    "        src = self.pos_encoder(src)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        output = self.transformer(src, tgt)\n",
    "        output = self.fc_out(output)\n",
    "        return output\n",
    "\n",
    "# Определяем PositionalEncoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Гиперпараметры (должны совпадать с обученной моделью)\n",
    "vocab_size = 762\n",
    "embed_dim = 128\n",
    "nhead = 4\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "dropout = 0.1\n",
    "\n",
    "# Инициализируем модель\n",
    "model = TransformerModel(vocab_size, embed_dim, nhead, hidden_dim, num_layers, dropout)\n",
    "\n",
    "# Загружаем сохранённые веса\n",
    "model.load_state_dict(torch.load(\"/content/model_weights.pth\", map_location=torch.device('cpu')))\n",
    "\n",
    "# Переводим в режим оценки\n",
    "model.eval()\n",
    "\n",
    "\n",
    "with open(\"vocab.json\", \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "\n",
    "vocab = {word: idx for idx, word in enumerate(vocab)}  \n",
    "vocab[\"<unk>\"] = 0  # Добавляем токен для неизвестных слов\n",
    "\n",
    "# Обратный словарь\n",
    "reverse_vocab = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "# Токенизатор (преобразует строку в индексы)\n",
    "def tokenizer(text):\n",
    "    return [vocab.get(word, vocab[\"<unk>\"]) for word in text.split()]\n",
    "\n",
    "# Детокенизатор (преобразует индексы обратно в строку)\n",
    "def detokenizer(indices):\n",
    "    return \" \".join([reverse_vocab.get(idx, \"<unk>\") for idx in indices])\n",
    "\n",
    "\n",
    "# Загрузка модели\n",
    "model = torch.load(\"model.pth\", map_location=torch.device(\"cpu\"))\n",
    "model.eval()\n",
    "\n",
    "# Загрузка словаря\n",
    "with open(\"vocab.json\", \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "vocab = {word: idx for idx, word in enumerate(vocab)}  # Индексация\n",
    "vocab[\"<unk>\"] = 0  # Добавляем токен для неизвестных слов\n",
    "reverse_vocab = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "# Функции токенизатора\n",
    "def tokenizer(text):\n",
    "    return [vocab.get(word, vocab[\"<unk>\"]) for word in text.split()]\n",
    "\n",
    "def detokenizer(indices):\n",
    "    return \" \".join([reverse_vocab.get(idx, \"<unk>\") for idx in indices])\n",
    "\n",
    "# Функция генерации текста\n",
    "def generate_text(start_sequence, max_len=50, temperature=1.0, top_p=0.9):\n",
    "    tokens = tokenizer(start_sequence)\n",
    "    input_seq = torch.tensor(tokens).unsqueeze(0)\n",
    "    generated = tokens.copy()\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            output = model(input_seq, input_seq)\n",
    "            next_token_logits = output[0, -1, :]\n",
    "        next_token_logits = next_token_logits / temperature\n",
    "        sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        cutoff = torch.where(cumulative_probs > top_p)[0][0]\n",
    "        filtered_logits = sorted_logits[:cutoff + 1]\n",
    "        filtered_indices = sorted_indices[:cutoff + 1]\n",
    "        probs = F.softmax(filtered_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "        generated.append(filtered_indices[next_token].item())\n",
    "        input_seq = torch.tensor(generated).unsqueeze(0)\n",
    "        if filtered_indices[next_token].item() == vocab.get('<eos>', -1):\n",
    "            break\n",
    "    return detokenizer(generated)\n",
    "\n",
    "# Функции Телеграм-бота\n",
    "def start(update: Update, context: CallbackContext) -> None:\n",
    "    keyboard = [[\"Сгенерировать текст\"]]\n",
    "    reply_markup = ReplyKeyboardMarkup(keyboard, one_time_keyboard=True, resize_keyboard=True)\n",
    "    update.message.reply_text(\"Привет! Нажми кнопку, чтобы сгенерировать текст.\", reply_markup=reply_markup)\n",
    "\n",
    "def handle_message(update: Update, context: CallbackContext) -> None:\n",
    "    text = update.message.text\n",
    "    if text == \"Сгенерировать текст\":\n",
    "        generated_text = generate_text(\"adventurer\", max_len=30, temperature=0.7, top_p=0.9)\n",
    "        update.message.reply_text(f\"Сгенерированный текст: {generated_text}\")\n",
    "    else:\n",
    "        update.message.reply_text(\"Нажмите кнопку, чтобы сгенерировать текст.\")\n",
    "\n",
    "# Запуск бота\n",
    "def main():\n",
    "    updater = Updater(\"TOKEN\")\n",
    "    dp = updater.dispatcher\n",
    "    dp.add_handler(CommandHandler(\"start\", start))\n",
    "    dp.add_handler(MessageHandler(Filters.text & ~Filters.command, handle_message))\n",
    "    updater.start_polling()\n",
    "    updater.idle()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
